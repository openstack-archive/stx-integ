From 19b1b6ef9bed07b2f2e2ea144bf72ebd975967f9 Mon Sep 17 00:00:00 2001
From: Daniel Badea <daniel.badea@windriver.com>
Date: Fri, 29 Mar 2019 09:10:46 +0000
Subject: [PATCH] prepare-osd-to-be-started-via-init-script.patch

---
 manifests/osd.pp | 16 +++++++++++++++-
 1 file changed, 15 insertions(+), 1 deletion(-)

diff --git a/manifests/osd.pp b/manifests/osd.pp
index 1f3d2c3..c51a445 100644
--- a/manifests/osd.pp
+++ b/manifests/osd.pp
@@ -54,6 +54,7 @@ define ceph::osd (
   $cluster = undef,
   $cluster_uuid = undef,
   $uuid = undef,
+  $osdid = undef,
   $exec_timeout = $::ceph::params::exec_timeout,
   $selinux_file_context = 'ceph_var_lib_t',
   $fsid = undef,
@@ -78,6 +79,10 @@ define ceph::osd (
       $uuid_option = "--osd-uuid ${uuid}"
     }
 
+    if $osdid {
+      $osdid_option = "--osd-id ${osdid}"
+    }
+
     if $ensure == present {
 
       $ceph_check_udev = "ceph-osd-check-udev-${name}"
@@ -131,7 +136,16 @@ test -z $(ceph-disk list $(readlink -f ${data}) | egrep -o '[0-9a-f]{8}-([0-9a-f
       # ceph-disk: prepare should be idempotent http://tracker.ceph.com/issues/7475
       exec { $ceph_prepare:
 
-        command   => "/usr/sbin/ceph-disk --verbose --log-stdout prepare --filestore  ${cluster_uuid_option} ${uuid_option} --fs-type xfs --zap-disk $(readlink -f ${data}) $(readlink -f ${journal})",
+        command   => "/bin/true # comment to satisfy puppet syntax requirements
+set -ex
+ceph-disk --verbose --log-stdout prepare --filestore  ${cluster_uuid_option} ${uuid_option} ${osdid_option} --fs-type xfs --zap-disk $(readlink -f ${data}) $(readlink -f ${journal})
+mkdir -p /var/lib/ceph/osd/ceph-${osdid}
+ceph auth del osd.${osdid} || true
+mount $(readlink -f ${data})1 /var/lib/ceph/osd/ceph-${osdid}
+ceph-osd --id ${osdid} --mkfs --mkkey --mkjournal
+ceph auth add osd.${osdid} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-${osdid}/keyring
+umount /var/lib/ceph/osd/ceph-${osdid}
+",
         # We don't want to erase the disk if:
         # 1. There is already ceph data on the disk for our cluster AND
         # 2. The uuid for the OSD we are configuring matches the uuid for the
-- 
1.8.3.1

